# ğŸ¤– AI Agentic Project

An AI-powered multi-agent system that searches the web, generates detailed answers using Gemini, and refines them using Groqâ€™s LLaMA models. The system is designed to run locally using Docker and requires no manual setup once the image is built.

---

## ğŸš€ Features

* ğŸ” Web Search using Tavily API
* ğŸ§  Answer Generation via Google Gemini (Gemini 2.0 Flash)
* âœ¨ Answer Refinement via Groq LLaMA (llama-3.1-8b-instant)
* ğŸ³ Dockerized for easy deployment
* ğŸ” Optional: Secure `.env` configuration

---

## ğŸ“ Project Structure

```
ai_agentic_project/
â”‚
â”œâ”€â”€ main.py                # Main logic for multi-agent flow
â”œâ”€â”€ requirements.txt       # All dependencies
â”œâ”€â”€ Dockerfile             # Docker setup
â”œâ”€â”€ .env (optional)        # Contains API keys (not pushed to GitHub)
â”œâ”€â”€ README.md              # This file
â”œâ”€â”€ main.ipynb             # Jupyter notebook (optional usage)
â”œâ”€â”€ Test_prompt.txt        # Sample prompts for testing
â””â”€â”€ .gitignore             # Ignore sensitive or system files
```

---

## ğŸ§  Agents

### ğŸ”¹ Gemini Agent

* Uses `langchain_google_genai` and `gemini-2.0-flash`
* Generates detailed answers from Tavily search results

### ğŸ”¹ Groq Agent

* Uses `langchain_groq` with `llama-3.1-8b-instant`
* Refines Geminiâ€™s output into final polished response

---

## ğŸ› ï¸ Requirements

* Python 3.8+
* Docker installed

---

## ğŸ”‘ Environment Variables

You can either:

* Pass environment variables with `--env` flags (safe method), **or**
* Hardcode them in the script (less secure, but portable via Docker image)

### .env format (optional):

```env
GROQ_API_KEY=your-groq-key
TAVILY_API_KEY=your-tavily-key
GEMINI_API_KEY=your-gemini-key
```

---

## ğŸ³ Docker Setup

### ğŸ”¨ Step 1: Build the Docker Image

```bash
docker build -t mithun1701/my-llm-app .
```

### â™»ï¸ Step 2: Run the Container (with environment variables)

```bash
docker run -it --rm \
  -e GROQ_API_KEY=your-groq-key \
  -e TAVILY_API_KEY=your-tavily-key \
  -e GEMINI_API_KEY=your-gemini-key \
  mithun1701/my-llm-app:latest
```

### âœ… OR: Run Without .env (if hardcoded inside image)

```bash
docker pull mithun1701/my-llm-app:latest
docker run -it --rm mithun1701/my-llm-app:latest
```

---

## ğŸ”„ Updating the Docker Image (after code changes)

```bash
docker build -t mithun1701/my-llm-app .
docker push mithun1701/my-llm-app:latest
```

---

## ğŸ§ª Sample Output

```bash
ğŸ’¬ Enter your query:
> What is LangChain?

ğŸ” Searching the web...

ğŸ§  Generating detailed answer with Gemini...

âœ¨ Refining answer with Mistral...

âœ… FINAL OUTPUT:
Here is a polished and refined version of your response:
"LangChain is an open-source framework for building applications with LLMs..."
```

---

## ğŸ‘¨â€ğŸ’» Author

**Mithun** â€” [GitHub](https://github.com/mithun1701)
ğŸŒ Powered by Gemini + Groq + Tavily + LangChain

---

## âš ï¸ Disclaimer

This project is intended for educational/research purposes. Do **not** hardcode sensitive API keys in production Docker images or commit `.env` files.

---

## ğŸ“„ License

MIT License â€” Feel free to use, modify, and share ğŸš€
