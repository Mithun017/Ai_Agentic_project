{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2508f4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MITHUN\\AppData\\Local\\Temp\\ipykernel_2908\\191708281.py:21: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  ollama_llm = Ollama(model=\"mistral\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Searching the web...\n",
      "üß† Generating detailed answer with Gemini...\n",
      "‚ú® Refining answer with Mistral...\n",
      "\n",
      "‚úÖ FINAL OUTPUT:\n",
      " Title: Understanding the Versatile Greeting \"Hi\" in English Language\n",
      "\n",
      "Introduction:\n",
      "The salutation \"Hi\" is a common and versatile greeting in the English language. This article aims to shed light on its usage, characteristics, synonyms, and origin.\n",
      "\n",
      "General Usage:\n",
      "- \"Hi\" is suitable for most situations, excluding the most formal ones. For instance, it's appropriate to use \"Hi\" when passing someone on the street, but it may not be ideal when meeting the Queen of England.\n",
      "- It is less formal than \"hello.\"\n",
      "- \"Hi\" is a very common greeting.\n",
      "\n",
      "Formality:\n",
      "- Some sources suggest that \"Hi\" functions as the American equivalent of the British \"Hello,\" implying a level of formality when greeting strangers in America.\n",
      "- \"Hey\" is considered more casual and is often used with friends and family.\n",
      "- \"Hello\" is the most formal of the three.\n",
      "\n",
      "Pronunciation:\n",
      "- The pronunciation of \"hi\" is /ha…™/ in both US and UK English.\n",
      "\n",
      "Synonyms:\n",
      "- Synonyms for \"hi\" include \"hello,\" \"how-do-you-do,\" \"howdy,\" and \"hullo.\"\n",
      "\n",
      "Origin:\n",
      "- One source attributes the first use of \"hi\" to a Kansas native American in the 1800s.\n",
      "\n",
      "Other Uses:\n",
      "- \"Hi\" can also function as an adjective, such as in the term \"hi fidelity.\"\n",
      "- \"HI\" is also an abbreviation.\n",
      "\n",
      "Conclusion:\n",
      "In conclusion, \"Hi\" is a versatile and widely used greeting in English, with various levels of formality depending on the context and relationship between individuals. Whether you're passing someone on the street or meeting a stranger, understanding when to use \"Hi\" can help ensure smooth interactions and a friendly demeanor.\n"
     ]
    }
   ],
   "source": [
    "from tavily import TavilyClient\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# API keys\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "tavily_api_key = os.getenv(\"TAVILY_API_KEY\")\n",
    "\n",
    "# Validate API keys\n",
    "if not gemini_api_key or not tavily_api_key:\n",
    "    raise ValueError(\"Missing GEMINI_API_KEY or TAVILY_API_KEY in .env file\")\n",
    "\n",
    "# Initialize LLMs\n",
    "gemini_llm = ChatGoogleGenerativeAI(api_key=gemini_api_key, model=\"gemini-2.0-flash\")\n",
    "ollama_llm = Ollama(model=\"mistral\")\n",
    "\n",
    "# Tavily search\n",
    "def tavily_search(query: str) -> str:\n",
    "    client = TavilyClient(api_key=tavily_api_key)\n",
    "    results = client.search(query=query, search_depth=\"advanced\")\n",
    "    return \"\\n\".join([r['content'] for r in results.get('results', [])[:3]])\n",
    "\n",
    "# Prompt templates\n",
    "gemini_prompt_template = PromptTemplate.from_template(\"\"\"\n",
    "You are a knowledgeable assistant. Based on the following search results, write a comprehensive and informative answer to the user's query.\n",
    "\n",
    "Search Results:\n",
    "{search_results}\n",
    "\n",
    "User Query:\n",
    "{user_query}\n",
    "\"\"\")\n",
    "\n",
    "mistral_prompt_template = PromptTemplate.from_template(\"\"\"\n",
    "You are an expert editor. Polish and refine the following draft into a final high-quality answer:\n",
    "\n",
    "Draft:\n",
    "{draft}\n",
    "\"\"\")\n",
    "\n",
    "# Main logic\n",
    "def main():\n",
    "    user_query = input(\"üí¨ Enter your query: \")\n",
    "\n",
    "    print(\"üîç Searching the web...\")\n",
    "    search_results = tavily_search(user_query)\n",
    "\n",
    "    print(\"üß† Generating detailed answer with Gemini...\")\n",
    "    gemini_prompt = gemini_prompt_template.format(search_results=search_results, user_query=user_query)\n",
    "    gemini_output = gemini_llm.invoke(gemini_prompt)\n",
    "\n",
    "    print(\"‚ú® Refining answer with Mistral...\")\n",
    "    mistral_prompt = mistral_prompt_template.format(draft=gemini_output)\n",
    "    final_output = ollama_llm.invoke(mistral_prompt)\n",
    "\n",
    "    print(\"\\n‚úÖ FINAL OUTPUT:\")\n",
    "    print(final_output)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1df5011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Searching the web...\n",
      "\n",
      "üß† Generating detailed answer with Gemini...\n",
      "\n",
      "‚ú® Refining answer with Mistral...\n",
      "\n",
      "\n",
      "‚úÖ FINAL OUTPUT:\n",
      "Here is a polished and refined version of your response:\n",
      "\n",
      "Here's the refined draft:\n",
      "\n",
      "The expression \"the sum of the series 2n+1\" is ambiguous without further context. It could refer to several distinct mathematical concepts:\n",
      "\n",
      "**1. The Sum of the First n Odd Natural Numbers:**\n",
      "\n",
      "If you are seeking the sum of the first n numbers in the sequence 1, 3, 5, 7, ..., (2n - 1) or (2n + 1), the formula to obtain this sum is:\n",
      "\n",
      "   Sum = n^2\n",
      "\n",
      "This formula holds true for the sum of the first n odd natural numbers, where n is a positive integer.\n",
      "\n",
      "**2. The nth Term of a Sequence:**\n",
      "\n",
      "If 2n + 1 represents the nth term of a sequence, then the given expression is not summing a series, but rather defining a sequence. In this case, the expression 2n + 1 would be used to determine the nth value of the sequence, rather than computing a sum.\n",
      "\n",
      "**3. Summation Notation:**\n",
      "\n",
      "If you intend to sum the expression (2i + 1) from i = 1 to n, the correct formula to compute this sum is:\n",
      "\n",
      "   ‚àë[i = 1 to n] (2i + 1) = n^2\n",
      "\n",
      "Here, the summation notation ‚àë is used to indicate the sum of the terms from i = 1 to n, which simplifies to n^2. This formula demonstrates that the sum of the first n odd natural numbers is indeed equal to n^2.\n"
     ]
    }
   ],
   "source": [
    "from tavily import TavilyClient\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# API keys\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "tavily_api_key = os.getenv(\"TAVILY_API_KEY\")\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# Validate API keys\n",
    "if not gemini_api_key or not tavily_api_key or not groq_api_key:\n",
    "    raise ValueError(\"‚ùå Missing API keys in .env file\")\n",
    "\n",
    "# Initialize LLMs\n",
    "gemini_llm = ChatGoogleGenerativeAI(api_key=gemini_api_key, model=\"gemini-2.0-flash\")\n",
    "groq_llm = ChatGroq(api_key=groq_api_key, model=\"llama-3.1-8b-instant\")\n",
    "\n",
    "# Tavily search function\n",
    "def tavily_search(query: str) -> str:\n",
    "    client = TavilyClient(api_key=tavily_api_key)\n",
    "    results = client.search(query=query, search_depth=\"advanced\")\n",
    "    return \"\\n\".join([r['content'] for r in results.get('results', [])[:3]])\n",
    "\n",
    "# Clean HTML tags from output (optional)\n",
    "def clean_output(text):\n",
    "    text = re.sub(r'<sub>(.*?)</sub>', r'_\\1_', text)\n",
    "    text = re.sub(r'<sup>(.*?)</sup>', r'^\\1^', text)\n",
    "    text = re.sub(r'<[^>]+>', '', text)  # remove any other HTML tags\n",
    "    return text.strip()\n",
    "\n",
    "# Prompt templates\n",
    "gemini_prompt_template = PromptTemplate.from_template(\"\"\"\n",
    "You are a knowledgeable assistant. Based on the following search results, write a comprehensive and informative answer to the user's query.\n",
    "\n",
    "Search Results:\n",
    "{search_results}\n",
    "\n",
    "User Query:\n",
    "{user_query}\n",
    "\"\"\")\n",
    "\n",
    "groq_prompt_template = PromptTemplate.from_template(\"\"\"\n",
    "You are a skilled language expert. Polish and refine the following draft into a final high-quality answer:\n",
    "\n",
    "Draft:\n",
    "{draft}\n",
    "\"\"\")\n",
    "\n",
    "# Main logic\n",
    "def main():\n",
    "    user_query = input(\"üí¨ Enter your query: \")\n",
    "\n",
    "    print(\"\\nüîç Searching the web...\\n\")\n",
    "    search_results = tavily_search(user_query)\n",
    "\n",
    "    print(\"üß† Generating detailed answer with Gemini...\\n\")\n",
    "    gemini_prompt = gemini_prompt_template.format(search_results=search_results, user_query=user_query)\n",
    "    gemini_output = gemini_llm.invoke(gemini_prompt)\n",
    "\n",
    "    print(\"‚ú® Refining answer with Mistral...\\n\")\n",
    "    groq_prompt = groq_prompt_template.format(draft=gemini_output.content)\n",
    "    final_output = groq_llm.invoke(groq_prompt)\n",
    "\n",
    "    print(\"\\n‚úÖ FINAL OUTPUT:\\nHere is a polished and refined version of your response:\\n\")\n",
    "    print(clean_output(final_output.content))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e0caba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîé Searching the web...\n",
      "\n",
      "ü§ñ Building Agent Response with Gemini...\n",
      "\n",
      "üìù Polishing Response with Groq (LLaMA 3)...\n",
      "\n",
      "\n",
      "‚úÖ FINAL OUTPUT:\n",
      "\n",
      "**Navigating the Complexities of Love: Advice for Your Friend**\n",
      "\n",
      "As you navigate the intricacies of being in love with multiple people, it's essential to acknowledge the uniqueness of your situation. While search results may not directly address being in love with ten individuals, the core principles of self-awareness, communication, and investment can be extrapolated from general relationship advice.\n",
      "\n",
      "**Foundational Principles: A Starting Point**\n",
      "\n",
      "1.  **Love Yourself First**: Prioritize self-love and self-awareness. Understanding your feelings, needs, and desires is crucial in navigating complex relationships.\n",
      "2.  **Paying Attention**: Spend quality time with each person, listen actively, and understand their values, dreams, and needs. This will help discern the depth of the connection.\n",
      "3.  **Investment**: Relationships require investment of time and effort to nurture and grow. Be prepared to put in the work to maintain connections.\n",
      "4.  **Prioritization**: With multiple relationships, prioritization is key. Focus on the connections that feel most meaningful and sustainable.\n",
      "\n",
      "**Evaluating Relationships: A Framework for Decision-Making**\n",
      "\n",
      "1.  **Individual Assessment**: Evaluate each connection individually, considering whether it's an infatuation, deep friendship, or genuine romantic feeling.\n",
      "2.  **Compatibility**: Assess which individuals share similar values, interests, and life goals.\n",
      "3.  **Reciprocity**: Evaluate who reciprocates the feelings and effort. Healthy relationships require mutual investment.\n",
      "4.  **Long-Term Potential**: Consider which connections feel sustainable and fulfilling in the long run.\n",
      "5.  **The \"Second Person\" Heuristic**: While initially designed for a love triangle, this advice can be adapted. Consider the relationships that developed after the initial ones, as these might indicate a shift in your needs or desires.\n",
      "\n",
      "**Communicating Your Intentions and Expectations**\n",
      "\n",
      "1.  **Honest Communication**: Be upfront and honest with everyone involved (as much as possible). Clearly communicate your intentions and expectations.\n",
      "2.  **Realistic Expectations**: Understand that it's unlikely to maintain ten deep, romantic relationships simultaneously. Be prepared for some connections to naturally fade as priorities become clearer.\n",
      "\n",
      "**Seeking Guidance: A Support System**\n",
      "\n",
      "1.  **Professional Guidance**: Consider seeking guidance from a therapist or counselor experienced in relationship dynamics, especially non-traditional relationships. They can provide personalized support and strategies.\n",
      "\n",
      "In conclusion, navigating the complexities of love requires self-awareness, communication, and investment. By prioritizing your relationships and being honest with yourself and others, you can build meaningful connections that bring joy and fulfillment to your life.\n"
     ]
    }
   ],
   "source": [
    "from tavily import TavilyClient\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# API keys\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "tavily_api_key = os.getenv(\"TAVILY_API_KEY\")\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# Validate API keys\n",
    "if not gemini_api_key or not tavily_api_key or not groq_api_key:\n",
    "    raise ValueError(\"‚ùå Missing API keys in .env file\")\n",
    "\n",
    "# Initialize LLMs\n",
    "gemini_llm = ChatGoogleGenerativeAI(api_key=gemini_api_key, model=\"gemini-2.0-flash\")\n",
    "groq_llm = ChatGroq(api_key=groq_api_key, model=\"llama-3.1-8b-instant\")\n",
    "\n",
    "# Tavily web search\n",
    "def tavily_search(query: str) -> str:\n",
    "    client = TavilyClient(api_key=tavily_api_key)\n",
    "    results = client.search(query=query, search_depth=\"advanced\")\n",
    "    return \"\\n\".join([r['content'] for r in results.get('results', [])[:3]])\n",
    "\n",
    "# Clean HTML tags from output\n",
    "def clean_output(text):\n",
    "    text = re.sub(r'<sub>(.*?)</sub>', r'_\\1_', text)\n",
    "    text = re.sub(r'<sup>(.*?)</sup>', r'^\\1^', text)\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "# Enhanced Gemini prompt (acts as intelligent agent)\n",
    "gemini_prompt_template = PromptTemplate.from_template(\"\"\"\n",
    "You are an intelligent research assistant helping a user solve a complex question.\n",
    "\n",
    "Your task is to:\n",
    "1. Analyze the web search results provided.\n",
    "2. Extract key facts, evidence, and insights.\n",
    "3. Use reasoning and deduction to generate a clear, structured, and comprehensive response to the user's query.\n",
    "\n",
    "Format your response with:\n",
    "- üîç Insight Summary\n",
    "- üìå Key Points\n",
    "- üß† Agent‚Äôs Thought Process\n",
    "- ‚úÖ Final Answer\n",
    "\n",
    "Web Search Results:\n",
    "{search_results}\n",
    "\n",
    "User Query:\n",
    "{user_query}\n",
    "\n",
    "Be accurate, informative, and helpful. Avoid assumptions not supported by the search results.\n",
    "\"\"\")\n",
    "\n",
    "# Enhanced Groq prompt (polishes final response)\n",
    "groq_prompt_template = PromptTemplate.from_template(\"\"\"\n",
    "You are a language expert with excellent writing skills.\n",
    "\n",
    "Take the following AI-generated draft, and:\n",
    "- Improve clarity and structure.\n",
    "- Polish grammar, punctuation, and coherence.\n",
    "- Make the response more engaging and human-readable, without changing the core meaning.\n",
    "\n",
    "Respond with a clear, final version of the answer.\n",
    "\n",
    "Draft Response:\n",
    "{draft}\n",
    "\"\"\")\n",
    "\n",
    "# Main logic\n",
    "def main():\n",
    "    user_query = input(\"Enter your query: \")\n",
    "\n",
    "    print(\"\\nüîé Searching the web...\\n\")\n",
    "    search_results = tavily_search(user_query)\n",
    "\n",
    "    print(\"ü§ñ Building Agent Response with Gemini...\\n\")\n",
    "    gemini_prompt = gemini_prompt_template.format(search_results=search_results, user_query=user_query)\n",
    "    gemini_output = gemini_llm.invoke(gemini_prompt)\n",
    "\n",
    "    print(\"üìù Polishing Response with Groq (LLaMA 3)...\\n\")\n",
    "    groq_prompt = groq_prompt_template.format(draft=gemini_output.content)\n",
    "    final_output = groq_llm.invoke(groq_prompt)\n",
    "\n",
    "    print(\"\\n‚úÖ FINAL OUTPUT:\\n\")\n",
    "    print(clean_output(final_output.content))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42d628d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Detected Emotion: Neutral\n",
      "\n",
      "üîé Searching the web...\n",
      "\n",
      "ü§ñ Creating intelligent agent response using Gemini Flash 2.0...\n",
      "\n",
      "üéØ Polishing the response using Groq (LLaMA 3.1)...\n",
      "\n",
      "‚úÖ FINAL OUTPUT:\n",
      "\n",
      "**Unlocking the Power of Clustering in Machine Learning**\n",
      "\n",
      "**üîç Insight Summary:**\n",
      "\n",
      "Imagine you're trying to make sense of a vast, unorganized dataset. You want to identify patterns, trends, and relationships between seemingly unrelated data points. This is where clustering comes in ‚Äì a powerful machine learning technique that groups similar data points into clusters based on defined similarity measures. The ultimate goal is to maximize similarity within clusters and minimize similarity between them, allowing you to uncover hidden insights and make informed decisions.\n",
      "\n",
      "**üìå Key Points:**\n",
      "\n",
      "* Clustering is a data-driven approach that groups unlabeled data into clusters based on similarity measures.\n",
      "* The primary objective is to maximize intra-cluster similarity and minimize inter-cluster similarity, resulting in well-defined clusters.\n",
      "* Clustering is a valuable tool for exploratory data analysis, data preprocessing, and pattern discovery.\n",
      "* Hierarchical clustering is a specific approach that iteratively merges the closest clusters until all data points form a single cluster.\n",
      "\n",
      "**üí° Understanding Clustering:**\n",
      "\n",
      "As we explore the concept of clustering, it's essential to understand the underlying process. The algorithm analyzes your data, identifies similar patterns, and groups them accordingly. Think of it as organizing a messy room ‚Äì you categorize similar items together, creating a sense of order and structure. This process enables you to:\n",
      "\n",
      "* Discover hidden patterns and relationships within your data\n",
      "* Identify trends and anomalies\n",
      "* Prepare your data for other machine learning tasks\n",
      "* Gain valuable insights to inform business decisions\n",
      "\n",
      "**‚úÖ Final Answer:**\n",
      "\n",
      "Clustering in machine learning is a powerful technique that helps you group similar data points together, uncovering hidden patterns and relationships. By maximizing intra-cluster similarity and minimizing inter-cluster similarity, you can gain a deeper understanding of your data and make informed decisions. Whether you're exploring data, preprocessing, or discovering patterns, clustering is an essential tool in your machine learning toolkit.\n",
      "\n",
      "I made the following changes to enhance the structure and flow, polish grammar and clarity, and adapt the tone to the user's emotion:\n",
      "\n",
      "1. **Reorganized the structure**: I rearranged the content to create a clear flow of information, starting with an introduction, followed by key points, and concluding with a final answer.\n",
      "2. **Improved grammar and clarity**: I corrected minor errors and rephrased sentences to enhance clarity and readability.\n",
      "3. **Added engaging elements**: I incorporated emojis and metaphors to make the content more engaging and accessible.\n",
      "4. **Maintained a helpful and neutral tone**: I ensured that the tone remained informative, yet friendly and approachable, to provide a positive user experience.\n",
      "5. **Used concise language**: I used clear and concise language to convey complex concepts, making it easier for readers to understand and absorb the information.\n"
     ]
    }
   ],
   "source": [
    "from tavily import TavilyClient\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# API keys\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "tavily_api_key = os.getenv(\"TAVILY_API_KEY\")\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# Validate API keys\n",
    "if not gemini_api_key or not tavily_api_key or not groq_api_key:\n",
    "    raise ValueError(\"‚ùå Missing one or more API keys (GEMINI_API_KEY, TAVILY_API_KEY, GROQ_API_KEY) in .env file\")\n",
    "\n",
    "# Initialize language models\n",
    "gemini_llm = ChatGoogleGenerativeAI(api_key=gemini_api_key, model=\"gemini-2.0-flash\")\n",
    "groq_llm = ChatGroq(api_key=groq_api_key, model=\"llama-3.1-8b-instant\")\n",
    "\n",
    "# Tavily web search function\n",
    "def tavily_search(query: str) -> str:\n",
    "    client = TavilyClient(api_key=tavily_api_key)\n",
    "    results = client.search(query=query, search_depth=\"advanced\")\n",
    "    return \"\\n\".join([r['content'] for r in results.get('results', [])[:3]])\n",
    "\n",
    "# HTML cleaner\n",
    "def clean_output(text):\n",
    "    text = re.sub(r'<sub>(.*?)</sub>', r'_\\1_', text)\n",
    "    text = re.sub(r'<sup>(.*?)</sup>', r'^\\1^', text)\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "# Detect emotion in user query\n",
    "def detect_emotion(text: str) -> str:\n",
    "    emotion_map = {\n",
    "        \"sad\": [\"sad\", \"depressed\", \"unhappy\", \"frustrated\", \"disappointed\"],\n",
    "        \"angry\": [\"angry\", \"mad\", \"furious\", \"irritated\"],\n",
    "        \"happy\": [\"happy\", \"excited\", \"joyful\", \"glad\", \"pleased\"],\n",
    "        \"anxious\": [\"worried\", \"anxious\", \"nervous\", \"concerned\", \"afraid\"],\n",
    "        \"neutral\": []\n",
    "    }\n",
    "    text_lower = text.lower()\n",
    "    for emotion, keywords in emotion_map.items():\n",
    "        if any(word in text_lower for word in keywords):\n",
    "            return emotion\n",
    "    return \"neutral\"\n",
    "\n",
    "# Tone instruction based on emotion\n",
    "def tone_instruction(emotion: str) -> str:\n",
    "    tones = {\n",
    "        \"sad\": \"Be gentle and supportive in your tone. Offer encouragement.\",\n",
    "        \"angry\": \"Stay calm and professional. Provide constructive, de-escalating language.\",\n",
    "        \"happy\": \"Reflect their enthusiasm! Keep your response cheerful and positive.\",\n",
    "        \"anxious\": \"Reassure the user and give confident, clear guidance.\",\n",
    "        \"neutral\": \"Maintain a helpful and neutral tone.\"\n",
    "    }\n",
    "    return tones.get(emotion, tones[\"neutral\"])\n",
    "\n",
    "# Generate Gemini prompt\n",
    "def generate_gemini_prompt(search_results: str, user_query: str, emotion: str):\n",
    "    tone = tone_instruction(emotion)\n",
    "    return PromptTemplate.from_template(f\"\"\"\n",
    "You are a highly intelligent assistant helping a user solve a complex question.\n",
    "\n",
    "Steps:\n",
    "1. Analyze the following web search results.\n",
    "2. Extract key facts, insights, and evidence.\n",
    "3. Respond clearly, structured, and tailored to the user's emotional tone.\n",
    "\n",
    "Tone Guide: {tone}\n",
    "\n",
    "Structure your output like this:\n",
    "üîç Insight Summary  \n",
    "üìå Key Points  \n",
    "üß† Agent‚Äôs Thought Process  \n",
    "‚úÖ Final Answer (Empathetic or appropriate to emotional context)\n",
    "\n",
    "Search Results:\n",
    "{{search_results}}\n",
    "\n",
    "User Query:\n",
    "{{user_query}}\n",
    "\"\"\").format(search_results=search_results, user_query=user_query)\n",
    "\n",
    "# Generate Groq prompt\n",
    "def generate_groq_prompt(draft: str, emotion: str):\n",
    "    tone = tone_instruction(emotion)\n",
    "    return PromptTemplate.from_template(f\"\"\"\n",
    "You are a language expert with emotional intelligence and excellent writing skills.\n",
    "\n",
    "Improve the following draft by:\n",
    "- Enhancing structure and flow.\n",
    "- Polishing grammar and clarity.\n",
    "- Making the answer more engaging.\n",
    "- Adapting tone to the user's emotion: {tone}\n",
    "\n",
    "Draft Response:\n",
    "{{draft}}\n",
    "\"\"\").format(draft=draft)\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    user_query = input(\"üí¨ Enter your query: \")\n",
    "\n",
    "    # Emotion Detection\n",
    "    emotion = detect_emotion(user_query)\n",
    "    print(f\"\\nüß† Detected Emotion: {emotion.capitalize()}\")\n",
    "\n",
    "    # Web Search\n",
    "    print(\"\\nüîé Searching the web...\")\n",
    "    search_results = tavily_search(user_query)\n",
    "\n",
    "    # Generate Answer with Gemini\n",
    "    print(\"\\nü§ñ Creating intelligent agent response using Gemini Flash 2.0...\")\n",
    "    gemini_prompt = generate_gemini_prompt(search_results, user_query, emotion)\n",
    "    gemini_output = gemini_llm.invoke(gemini_prompt)\n",
    "\n",
    "    # Refine with Groq\n",
    "    print(\"\\nüéØ Polishing the response using Groq (LLaMA 3.1)...\")\n",
    "    groq_prompt = generate_groq_prompt(gemini_output.content, emotion)\n",
    "    final_output = groq_llm.invoke(groq_prompt)\n",
    "\n",
    "    # Final Output\n",
    "    print(\"\\n‚úÖ FINAL OUTPUT:\\n\")\n",
    "    print(clean_output(final_output.content))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0f0917",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tavily import TavilyClient\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Directly set API keys (‚ö†Ô∏è not recommended for production)\n",
    "gemini_api_key = \"AIzaSyDMjMNmOs--NydgDY4i6m9hWdNwQVKuRK4\"\n",
    "tavily_api_key = \"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\"\n",
    "groq_api_key = \"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\"\n",
    "\n",
    "# Validate API keys\n",
    "if not gemini_api_key or not tavily_api_key or not groq_api_key:\n",
    "    raise ValueError(\"‚ùå Missing one or more API keys (GEMINI_API_KEY, TAVILY_API_KEY, GROQ_API_KEY)\")\n",
    "\n",
    "# Initialize language models\n",
    "gemini_llm = ChatGoogleGenerativeAI(api_key=gemini_api_key, model=\"gemini-2.0-flash\")\n",
    "groq_llm = ChatGroq(api_key=groq_api_key, model=\"llama-3.1-8b-instant\")\n",
    "\n",
    "# Tavily web search function\n",
    "def tavily_search(query: str) -> str:\n",
    "    client = TavilyClient(api_key=tavily_api_key)\n",
    "    results = client.search(query=query, search_depth=\"advanced\")\n",
    "    return \"\\n\".join([r['content'] for r in results.get('results', [])[:3]])\n",
    "\n",
    "# HTML cleaner\n",
    "def clean_output(text):\n",
    "    text = re.sub(r'<sub>(.*?)</sub>', r'_\\1_', text)\n",
    "    text = re.sub(r'<sup>(.*?)</sup>', r'^\\1^', text)\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "# Detect emotion in user query\n",
    "def detect_emotion(text: str) -> str:\n",
    "    emotion_map = {\n",
    "        \"sad\": [\"sad\", \"depressed\", \"unhappy\", \"frustrated\", \"disappointed\"],\n",
    "        \"angry\": [\"angry\", \"mad\", \"furious\", \"irritated\"],\n",
    "        \"happy\": [\"happy\", \"excited\", \"joyful\", \"glad\", \"pleased\"],\n",
    "        \"anxious\": [\"worried\", \"anxious\", \"nervous\", \"concerned\", \"afraid\"],\n",
    "        \"neutral\": []\n",
    "    }\n",
    "    text_lower = text.lower()\n",
    "    for emotion, keywords in emotion_map.items():\n",
    "        if any(word in text_lower for word in keywords):\n",
    "            return emotion\n",
    "    return \"neutral\"\n",
    "\n",
    "# Tone instruction based on emotion\n",
    "def tone_instruction(emotion: str) -> str:\n",
    "    tones = {\n",
    "        \"sad\": \"Be gentle and supportive in your tone. Offer encouragement.\",\n",
    "        \"angry\": \"Stay calm and professional. Provide constructive, de-escalating language.\",\n",
    "        \"happy\": \"Reflect their enthusiasm! Keep your response cheerful and positive.\",\n",
    "        \"anxious\": \"Reassure the user and give confident, clear guidance.\",\n",
    "        \"neutral\": \"Maintain a helpful and neutral tone.\"\n",
    "    }\n",
    "    return tones.get(emotion, tones[\"neutral\"])\n",
    "\n",
    "# Generate Gemini prompt\n",
    "def generate_gemini_prompt(search_results: str, user_query: str, emotion: str):\n",
    "    tone = tone_instruction(emotion)\n",
    "    return PromptTemplate.from_template(f\"\"\"\n",
    "You are a highly intelligent assistant helping a user solve a complex question.\n",
    "\n",
    "Steps:\n",
    "1. Analyze the following web search results.\n",
    "2. Extract key facts, insights, and evidence.\n",
    "3. Respond clearly, structured, and tailored to the user's emotional tone.\n",
    "\n",
    "Tone Guide: {tone}\n",
    "\n",
    "Structure your output like this:\n",
    "üîç Insight Summary  \n",
    "üìå Key Points  \n",
    "üß† Agent‚Äôs Thought Process  \n",
    "‚úÖ Final Answer (Empathetic or appropriate to emotional context)\n",
    "\n",
    "Search Results:\n",
    "{{search_results}}\n",
    "\n",
    "User Query:\n",
    "{{user_query}}\n",
    "\"\"\").format(search_results=search_results, user_query=user_query)\n",
    "\n",
    "# Generate Groq prompt\n",
    "def generate_groq_prompt(draft: str, emotion: str):\n",
    "    tone = tone_instruction(emotion)\n",
    "    return PromptTemplate.from_template(f\"\"\"\n",
    "You are a language expert with emotional intelligence and excellent writing skills.\n",
    "\n",
    "Improve the following draft by:\n",
    "- Enhancing structure and flow.\n",
    "- Polishing grammar and clarity.\n",
    "- Making the answer more engaging.\n",
    "- Adapting tone to the user's emotion: {tone}\n",
    "\n",
    "Draft Response:\n",
    "{{draft}}\n",
    "\"\"\").format(draft=draft)\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    user_query = input(\"üí¨ Enter your query: \")\n",
    "\n",
    "    # Emotion Detection\n",
    "    emotion = detect_emotion(user_query)\n",
    "    print(f\"\\nüß† Detected Emotion: {emotion.capitalize()}\")\n",
    "\n",
    "    # Web Search\n",
    "    print(\"\\nüîé Searching the web...\")\n",
    "    search_results = tavily_search(user_query)\n",
    "\n",
    "    # Generate Answer with Gemini\n",
    "    print(\"\\nü§ñ Creating intelligent agent response using Gemini Flash 2.0...\")\n",
    "    gemini_prompt = generate_gemini_prompt(search_results, user_query, emotion)\n",
    "    gemini_output = gemini_llm.invoke(gemini_prompt)\n",
    "\n",
    "    # Refine with Groq\n",
    "    print(\"\\nüéØ Polishing the response using Groq (LLaMA 3.1)...\")\n",
    "    groq_prompt = generate_groq_prompt(gemini_output.content, emotion)\n",
    "    final_output = groq_llm.invoke(groq_prompt)\n",
    "\n",
    "    # Final Output\n",
    "    print(\"\\n‚úÖ FINAL OUTPUT:\\n\")\n",
    "    print(clean_output(final_output.content))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f192f2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from tavily import TavilyClient\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.globals import set_verbose  \n",
    "\n",
    "set_verbose(False)\n",
    "\n",
    "gemini_api_key = \"AIzaSyDMjMNmOs--NydgDY4i6m9hWdNwQVKuRK4\"\n",
    "tavily_api_key = \"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\"\n",
    "groq_api_key = \"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\"\n",
    "\n",
    "if not gemini_api_key or not tavily_api_key or not groq_api_key:\n",
    "    raise ValueError(\"‚ùå Missing one or more API keys.\")\n",
    "\n",
    "gemini_llm = ChatGoogleGenerativeAI(api_key=gemini_api_key, model=\"gemini-2.0-flash\")\n",
    "groq_llm = ChatGroq(api_key=groq_api_key, model=\"llama-3.1-8b-instant\")\n",
    "\n",
    "def tavily_search(query: str) -> str:\n",
    "    client = TavilyClient(api_key=tavily_api_key)\n",
    "    results = client.search(query=query, search_depth=\"advanced\")\n",
    "    return \"\\n\".join([r['content'] for r in results.get('results', [])[:3]])\n",
    "\n",
    "def clean_output(text):\n",
    "    text = re.sub(r'<sub>(.*?)</sub>', r'_\\1_', text)\n",
    "    text = re.sub(r'<sup>(.*?)</sup>', r'^\\1^', text)\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "def detect_emotion(text: str) -> str:\n",
    "    emotion_map = {\n",
    "        \"sad\": [\"sad\", \"depressed\", \"unhappy\", \"frustrated\", \"disappointed\"],\n",
    "        \"angry\": [\"angry\", \"mad\", \"furious\", \"irritated\"],\n",
    "        \"happy\": [\"happy\", \"excited\", \"joyful\", \"glad\", \"pleased\"],\n",
    "        \"anxious\": [\"worried\", \"anxious\", \"nervous\", \"concerned\", \"afraid\"],\n",
    "        \"neutral\": []\n",
    "    }\n",
    "    text_lower = text.lower()\n",
    "    for emotion, keywords in emotion_map.items():\n",
    "        if any(word in text_lower for word in keywords):\n",
    "            return emotion\n",
    "    return \"neutral\"\n",
    "\n",
    "def tone_instruction(emotion: str) -> str:\n",
    "    tones = {\n",
    "        \"sad\": \"Be gentle and supportive in your tone. Offer encouragement.\",\n",
    "        \"angry\": \"Stay calm and professional. Provide constructive, de-escalating language.\",\n",
    "        \"happy\": \"Reflect their enthusiasm! Keep your response cheerful and positive.\",\n",
    "        \"anxious\": \"Reassure the user and give confident, clear guidance.\",\n",
    "        \"neutral\": \"Maintain a helpful and neutral tone.\"\n",
    "    }\n",
    "    return tones.get(emotion, tones[\"neutral\"])\n",
    "\n",
    "def generate_gemini_prompt(search_results: str, user_query: str, emotion: str):\n",
    "    tone = tone_instruction(emotion)\n",
    "    return PromptTemplate.from_template(f\"\"\"\n",
    "You are a highly intelligent assistant helping a user solve a complex question.\n",
    "\n",
    "Steps:\n",
    "1. Analyze the following web search results.\n",
    "2. Extract key facts, insights, and evidence.\n",
    "3. Respond clearly, structured, and tailored to the user's emotional tone.\n",
    "\n",
    "Tone Guide: {tone}\n",
    "\n",
    "Structure your output like this:\n",
    "üîç Insight Summary  \n",
    "üìå Key Points  \n",
    "üß† Agent‚Äôs Thought Process  \n",
    "‚úÖ Final Answer (Empathetic or appropriate to emotional context)\n",
    "\n",
    "Search Results:\n",
    "{{search_results}}\n",
    "\n",
    "User Query:\n",
    "{{user_query}}\n",
    "\"\"\").format(search_results=search_results, user_query=user_query)\n",
    "\n",
    "def generate_groq_prompt(draft: str, emotion: str):\n",
    "    tone = tone_instruction(emotion)\n",
    "    return PromptTemplate.from_template(f\"\"\"\n",
    "You are a language expert with emotional intelligence and excellent writing skills.\n",
    "\n",
    "Improve the following draft by:\n",
    "- Enhancing structure and flow.\n",
    "- Polishing grammar and clarity.\n",
    "- Making the answer more engaging.\n",
    "- Adapting tone to the user's emotion: {tone}\n",
    "\n",
    "Draft Response:\n",
    "{{draft}}\n",
    "\"\"\").format(draft=draft)\n",
    "\n",
    "st.set_page_config(page_title=\"üß† Agentic AI Assistant\", layout=\"wide\")\n",
    "\n",
    "st.markdown(\n",
    "    \"\"\"\n",
    "    <style>\n",
    "        /* General styling */\n",
    "        body {\n",
    "            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;\n",
    "            background-color: #1a1a1a;\n",
    "            color: #d1d5db;\n",
    "        }\n",
    "        .main-container {\n",
    "            max-width: 800px;\n",
    "            margin: 0 auto;\n",
    "            padding: 20px;\n",
    "        }\n",
    "        /* Header */\n",
    "        .header {\n",
    "            text-align: center;\n",
    "            margin-bottom: 40px;\n",
    "        }\n",
    "        .header h1 {\n",
    "            font-size: 28px;\n",
    "            color: #e0e0e0;\n",
    "        }\n",
    "        .header p {\n",
    "            font-size: 14px;\n",
    "            color: #a0a0a0;\n",
    "        }\n",
    "        /* Bottom input bar */\n",
    "        .bottom-input {\n",
    "            position: fixed;\n",
    "            bottom: 20px;\n",
    "            left: 50px;\n",
    "            right: 50px;\n",
    "            background-color: #2a2a2a;\n",
    "            padding: 10px;\n",
    "            border-radius: 12px;\n",
    "            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.3);\n",
    "            z-index: 1000;\n",
    "            display: flex;\n",
    "            align-items: center;\n",
    "            border: 1px solid #3a3a3a;\n",
    "        }\n",
    "        .stTextInput input {\n",
    "            border: none;\n",
    "            background-color: #333;\n",
    "            color: #d1d5db;\n",
    "            font-size: 16px;\n",
    "            padding: 12px;\n",
    "            width: 100%;\n",
    "            outline: none;\n",
    "            border-radius: 8px;\n",
    "        }\n",
    "        .send-button {\n",
    "            background-color: #10a37f;\n",
    "            color: white;\n",
    "            border: none;\n",
    "            border-radius: 8px;\n",
    "            padding: 10px 16px;\n",
    "            font-size: 16px;\n",
    "            cursor: pointer;\n",
    "            transition: background-color 0.2s ease;\n",
    "            margin-left: 10px;\n",
    "        }\n",
    "        .send-button:hover {\n",
    "            background-color: #0e8c6b;\n",
    "        }\n",
    "        /* Response card */\n",
    "        .response-card {\n",
    "            background-color: #2a2a2a;\n",
    "            padding: 20px;\n",
    "            border-radius: 12px;\n",
    "            margin-bottom: 20px;\n",
    "            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.3);\n",
    "            border-left: 4px solid #10a37f;\n",
    "            font-size: 16px;\n",
    "            line-height: 1.6;\n",
    "            white-space: pre-wrap;\n",
    "            animation: fadeIn 0.3s ease-in;\n",
    "            color: #d1d5db;\n",
    "        }\n",
    "        @keyframes fadeIn {\n",
    "            from { opacity: 0; transform: translateY(10px); }\n",
    "            to { opacity: 1; transform: translateY(0); }\n",
    "        }\n",
    "        /* Spinner */\n",
    "        .stSpinner {\n",
    "            color: #10a37f;\n",
    "        }\n",
    "    </style>\n",
    "    \"\"\",\n",
    "    unsafe_allow_html=True\n",
    ")\n",
    "\n",
    "with st.container():\n",
    "    st.markdown('<div class=\"main-container\">', unsafe_allow_html=True)\n",
    "    \n",
    "    st.markdown(\n",
    "        \"\"\"\n",
    "        <div class=\"header\">\n",
    "            <h1>üß† Agentic AI Assistant</h1>\n",
    "            <p>Ask anything, and I'll research, think, and respond with clarity and empathy.</p>\n",
    "        </div>\n",
    "        \"\"\",\n",
    "        unsafe_allow_html=True\n",
    "    )\n",
    "\n",
    "    if \"last_response\" not in st.session_state:\n",
    "        st.session_state.last_response = \"\"\n",
    "\n",
    "    # Display response at the top\n",
    "    if st.session_state.last_response:\n",
    "        st.markdown(f'<div class=\"response-card\">{st.session_state.last_response}</div>', unsafe_allow_html=True)\n",
    "\n",
    "    # Bottom input bar\n",
    "    with st.form(key=\"chat_form\", clear_on_submit=True):\n",
    "        st.markdown('<div class=\"bottom-input\">', unsafe_allow_html=True)\n",
    "        col1, col2 = st.columns([10, 1])\n",
    "        with col1:\n",
    "            user_query = st.text_input(\"Your message\", label_visibility=\"collapsed\", placeholder=\"Type your question...\")\n",
    "        with col2:\n",
    "            submitted = st.form_submit_button(\"‚û§\", help=\"Send your query\")\n",
    "        st.markdown('</div>', unsafe_allow_html=True)\n",
    "\n",
    "    # Process user input\n",
    "    if submitted and user_query:\n",
    "        with st.spinner(\"üß† Detecting emotion...\"):\n",
    "            emotion = detect_emotion(user_query)\n",
    "\n",
    "        with st.spinner(\"üîé Searching the web...\"):\n",
    "            search_results = tavily_search(user_query)\n",
    "\n",
    "        with st.spinner(\"ü§ñ Drafting response...\"):\n",
    "            gemini_prompt = generate_gemini_prompt(search_results, user_query, emotion)\n",
    "            gemini_output = gemini_llm.invoke(gemini_prompt)\n",
    "\n",
    "        with st.spinner(\"üéØ Polishing response...\"):\n",
    "            groq_prompt = generate_groq_prompt(gemini_output.content, emotion)\n",
    "            final_output = groq_llm.invoke(groq_prompt)\n",
    "\n",
    "        st.session_state.last_response = clean_output(final_output.content)\n",
    "        st.rerun()\n",
    "\n",
    "    st.markdown('</div>', unsafe_allow_html=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
